---
title: E-commerce Showcase
description: Building an E‑commerce Analytics Pipeline with Bruin, PostgresSQL and DuckDB

navigation:
  icon: i-lucide-shopping-cart
---

:::code-tree{default-value=".bruin.yml"}
```yaml [.bruin.yml]
default_environment: default
environments:
    default:
        connections:
            postgres:
                - name: pg-default
                  username: postgres
                  password: postgres
                  host: localhost
                  port: 5432
                  database: postgres
            duckdb:
                - name: duckdb-default
                  path: duckdb.db
````

```toml [pyproject.toml]
[project]
name = "bruin"
version = "0.1.0"
requires-python = ">=3.13"
dependencies = [
    "faker>=37.11.0",
    "psycopg>=3.2.10",
]

[project.scripts]
generate-data = "generate_data:main"
```

```yaml [glossary.yml]
domains:
  customer-management:
    description: All aspects related to customer data, registration, and account management
    owners:
      - "Data Team"
    tags:
      - customer
  product-catalog:
    description: Product and catalog data including variants and SKUs
    owners:
      - "Data Team"
    tags:
      - product
      - catalog
  sales:
    description: Orders and order items lifecycle data
    owners:
      - "Data Team"
    tags:
      - sales

entities:
  Base:
    description: Required fields for every Ingestion/Staging step.
    attributes:
      ID:
        name: id
        type: integer
        primary_key: true
        description: The unique identifier in our systems.
        checks:
          - name: not_null
          - name: unique
      CreatedAt:
        name: created_at
        type: timestamp
        description: Timestamp when the record was created.
        checks:
          - name: not_null
      UpdatedAt:
        name: updated_at
        type: timestamp
        description: Timestamp when the record was last updated.
        checks:
          - name: not_null
  Customer:
    description: Customer is an individual/business registered on our platform.
    domains:
      - customer-management
    attributes:
      ID:
        name: customer_id
        type: integer
        checks:
          - name: not_null
          - name: unique
      Email:
        name: email
        type: string
        checks:
          - name: not_null
      FullName:
        name: full_name
        type: string
      Country:
        name: country
        type: string
      Age:
        name: age
        type: integer
        checks:
          - name: range
            min: 0
```

```yaml [orders-performance/pipeline.yml]
name: orders_performance

default_connections:
  duckdb: "duckdb-default"
  postgres: "pg-default"

variables:
  all_time:
    type: int
    default: 1 # 0 for all time - 1 to enable start_date and end_date
```

```yaml [orders-performance/assets/ingestion/raw.customers.asset.yml]
name: raw.customers
type: ingestr

description: Ingest OLTP customers from Postgres into DuckDB raw layer.

tags:
  - raw

parameters:
  source_connection: pg-default
  source_table: "public.customers"
  destination: duckdb
```

```yaml [orders-performance/assets/ingestion/raw.orders.asset.yml]
name: raw.orders
type: ingestr
description: Ingest OLTP orders from Postgres into DuckDB raw layer.

tags:
  - raw

parameters:
  destination: duckdb
  source_connection: pg-default
  source_table: public.orders
```

```sql [orders-performance/assets/mart/customers/mart.customers_by_age.asset.sql]
/* @bruin
name: mart.customers_by_age
type: duckdb.sql
tags:
  - mart
  - amazing-tag
materialization:
  type: table
  strategy: create+replace
depends:
  - stg.customers
owner: danielhe4rt@gmail.com
@bruin */

WITH src AS (
  SELECT CASE
            WHEN age < 25 THEN '18-24'
            WHEN age BETWEEN 25 AND 34 THEN '25-34'
            WHEN age BETWEEN 35 AND 49 THEN '35-49'
            ELSE '50+'
         END AS age_group
  FROM stg.customers
)
SELECT age_group,
       COUNT(*) AS total_customers
FROM src
GROUP BY age_group
ORDER BY total_customers DESC;
```

```sql [orders-performance/assets/mart/products/mart.product_performance.asset.sql]
/* @bruin
name: mart.product_performance
type: duckdb.sql
materialization:
  type: table
  strategy: create+replace
depends:
  - stg.products
  - stg.product_variants
  - stg.order_items
  - stg.orders
@bruin */

WITH paid_items AS (
  SELECT oi.*
  FROM stg.order_items oi
  JOIN stg.orders o ON o.order_id = oi.order_id
  WHERE o.status IN ('paid','shipped')
),
items_with_product AS (
  SELECT
    v.product_id,
    oi.quantity,
    oi.total_price
  FROM paid_items oi
  JOIN stg.product_variants v ON v.variant_id = oi.variant_id
)
SELECT
  p.product_id,
  p.name AS product_name,
  p.category,
  SUM(i.quantity) AS items_sold,
  SUM(i.total_price) AS gross_revenue,
  CASE WHEN SUM(i.quantity) = 0 THEN 0 ELSE SUM(i.total_price) / SUM(i.quantity) END AS avg_item_price
FROM items_with_product i
JOIN stg.products p ON p.product_id = i.product_id
GROUP BY 1,2,3
ORDER BY gross_revenue DESC;
```

```sql [orders-performance/assets/staging/stg.customers.asset.sql]
/* @bruin
name: stg.customers
type: duckdb.sql
materialization:
  type: table
depends:
  - raw.customers
@bruin */

SELECT
  id::INT AS customer_id,
  COALESCE(TRIM(email), '') AS email,
  COALESCE(TRIM(country), 'Unknown') AS country,
  COALESCE(age, 0) AS age,
  created_at,
  updated_at
FROM raw.customers
WHERE email IS NOT NULL;
```

:::


## **What this hands-on tutorial covers**

:::card-group{.w-full}
    ::::card
    ---
    icon: i-tabler-database-import
    target: _blank
    title: Ingestion Layer
  
    ---
    Learn how to configure Bruin assets for incremental ingestion from Postgres into DuckDB.
    ::::

    ::::card
    ---
    icon: i-tabler-transform
    target: _blank
    title: Transformation Layer
  
    ---
    Explore staging and mart patterns that turn raw e-commerce data into usable analytics.
    ::::

    ::::card
    ---
    icon: i-tabler-shield-check
    target: _blank
    title: Quality Checks
  
    ---
    Implement column-level and business logic validations to ensure data integrity.
    ::::

    ::::card
    ---
    icon: i-tabler-chart-bar
    target: _blank
    title: Analytics & Insights
  
    ---
    Build daily revenue and customer lifetime value dashboards from validated datasets.
    ::::
::: 


This tutorial is structured to give you a **straightforward, engineering-first path** to building a production-ready analytics pipeline. It’s about understanding how each component — ingestion, transformation, validation, and analysis — fits together under Bruin’s unified workflow.



## 1) Prerequisites & Setup

* Docker (for Postgres)
* Python 3.11+
* `uv` (fast Python toolchain manager) → [https://docs.astral.sh/uv/](https://docs.astral.sh/uv/)
* Git, Bash

```bash
git clone https://github.com/danielhe4rt/bruin-ecommerce-pipeline
cd bruin-ecommerce-pipeline
```

Or use the bruin templates directly from the CLI:

```bash
bruin init ecommerce

cd bruin
```

Then, open the project in your favorite code editor and let's get started!

## 2) Understanding the Data Model (OLTP → Analytics)


In e-commerce — and most analytics-driven businesses — data pipelines are typically organized into **layers**, each
serving a specific purpose. This layered structure keeps data **reliable, interpretable, and adaptable** as the business
scales.

The ultimate goal is to make data analysis effortless and focused on **answering real business questions** across key
domains, such as:

| Domain                            | Analysis Focus                                                                        |
| --------------------------------- | ------------------------------------------------------------------------------------- |
| **Customers**                     | Segmentation by age, geography, and behavior patterns                                 |
| **Products and Product Variants** | Performance by category, profitability, and pricing insights                          |
| **Orders and Order Items**        | Daily revenue, order volume, and purchasing trends                                    |
| **KPI layer**                     | Metrics like **AOV**, **units per order**, **top SKUs**, and **repeat purchase rate** |

By structuring data this way, teams can move from raw information to actionable insights with clarity and confidence.

At this tutorial, it's important to understand the data relationships that form the backbone of most e-commerce
systems. These relationships define how information flows across entities — from customers placing orders to
products being sold and analyzed.

Here’s the simplified schema we’ll use throughout this tutorial:

| From                 | To                           | Relationship Type |
| -------------------- | ---------------------------- | ----------------- |
| customers(id)        | orders(customer_id)          | 1:N (One-to-Many) |
| orders(id)           | order_items(order_id)        | 1:N (One-to-Many) |
| products(id)         | product_variants(product_id) | 1:N (One-to-Many) |
| product_variants(id) | order_items(variant_id)      | 1:N (One-to-Many) |



:::code-tree{default-value="customers.sql"} 
```sql [customers.sql]
CREATE TABLE IF NOT EXISTS customers (
    id          BIGSERIAL PRIMARY KEY,
    full_name   TEXT        NOT NULL,
    email       TEXT        NOT NULL UNIQUE,
    country     TEXT        NOT NULL,
    age         INTEGER     NOT NULL CHECK (age >= 0),
    created_at  TIMESTAMPTZ NOT NULL DEFAULT now(),
    updated_at  TIMESTAMPTZ NOT NULL DEFAULT now()
);
````

```sql [products.sql]
CREATE TABLE IF NOT EXISTS products (
    id          BIGSERIAL PRIMARY KEY,
    name        TEXT        NOT NULL,
    category    TEXT        NOT NULL,
    sku         TEXT        NOT NULL UNIQUE,
    created_at  TIMESTAMPTZ NOT NULL DEFAULT now(),
    updated_at  TIMESTAMPTZ NOT NULL DEFAULT now()
);
```

```sql [product_variants.sql]
CREATE TABLE IF NOT EXISTS product_variants (
    id                  BIGSERIAL PRIMARY KEY,
    product_id          BIGINT         NOT NULL REFERENCES products(id) ON DELETE CASCADE,
    variant_sku         TEXT           NOT NULL UNIQUE,
    color               TEXT,
    size                TEXT,
    manufacturing_price NUMERIC(10,2)  NOT NULL,
    selling_price       NUMERIC(10,2)  NOT NULL,
    stock_quantity      INTEGER        NOT NULL,
    is_active           BOOLEAN        NOT NULL DEFAULT TRUE,
    created_at          TIMESTAMPTZ    NOT NULL DEFAULT now(),
    updated_at          TIMESTAMPTZ    NOT NULL DEFAULT now()
);
```

```sql [orders.sql]
CREATE TABLE IF NOT EXISTS orders (
    id            BIGSERIAL PRIMARY KEY,
    customer_id   BIGINT      NOT NULL REFERENCES customers(id),
    order_date    TIMESTAMPTZ NOT NULL DEFAULT now(),
    status        TEXT        NOT NULL,
    total_amount  NUMERIC(12,2),
    created_at    TIMESTAMPTZ NOT NULL DEFAULT now(),
    updated_at    TIMESTAMPTZ NOT NULL DEFAULT now()
);
```

```sql [order_items.sql]
CREATE TABLE IF NOT EXISTS order_items (
    id           BIGSERIAL PRIMARY KEY,
    order_id     BIGINT         NOT NULL REFERENCES orders(id) ON DELETE CASCADE,
    variant_id   BIGINT         NOT NULL REFERENCES product_variants(id),
    quantity     INTEGER        NOT NULL,
    unit_price   NUMERIC(10,2)  NOT NULL,
    total_price  NUMERIC(12,2)  GENERATED ALWAYS AS (quantity * unit_price) STORED,
    created_at   TIMESTAMPTZ    NOT NULL DEFAULT now()
);
```

:::


This model mirrors a real-world e-commerce workflow:

- A customer makes one or more orders.
- Each order contains one or more items.
- Each item points to a specific product variant (like a red T-shirt, size M).
- Each variant belongs to a broader product (e.g., “Classic T-Shirt”).

These foreign key relationships allow us to join datasets efficiently and enable downstream analytics like:

| Domain    | Materialized Table                | Depends On                                                              |
| :-------- | --------------------------------- | ----------------------------------------------------------------------- |
| Customers | `mart.customers_by_age`           | `stg.customers`                                                         |
| Customers | `mart.customers_by_country`       | `stg.customers`                                                         |
| Products  | `mart.revenue_by_product`         | `raw.order_items`, `raw.product_variants`, `raw.products`               |
| Products  | `mart.revenue_by_category`        | `raw.order_items`, `raw.product_variants`, `raw.products`               |
| Products  | `mart.revenue_by_product_variant` | `stg.product_variants`, `stg.order_items`, `stg.orders`, `stg.products` |
| Sales     | `mart.sales_daily`                | `stg.order_items`, `stg.orders`                                         |

By establishing these links early, every layer of the pipeline — raw, staging, and mart — can remain consistent and
traceable across transformations.



## 3) Setting Up the Source Database (Docker + Postgres)

First, let's spin up our PostgreSQL instance:

:::code-group

```bash [Docker Run]
docker run -d \
  --name bruin-ecommerce \
  -e POSTGRES_USER=postgres \
  -e POSTGRES_PASSWORD=postgres \
  -e POSTGRES_DB=postgres \
  -p 5432:5432 \
  postgres:18-alpine
```

```bash [Docker Compose]
docker compose up -d
```

```bash [Makefile]
make docker-up
```
:::


Now, we must ingest the DDL inside the PostgresSQL instance:

::tabs

:::tabs-item{label="Terminal" icon="i-lucide-terminal"}

```bash 
cat sql/ddl.sql | docker exec -i bruin-ecommerce psql -U postgres -d postgres
```

:::

:::tabs-item{label="Terminal (Makefile)" icon="i-lucide-terminal"}

```bash
make db-migrate
```

:::

:::tabs-item{label="Output" icon="i-lucide-eye"}



```bash [Docker Exec]
CREATE TABLE
CREATE TABLE
CREATE TABLE
CREATE TABLE
CREATE TABLE
```

:::

::

::::note{.w-full.my-0}
  If you're using Makefile, check the commands available using `make help`
::::


## 4) Populating Test Data

Inside the project, you’ll find a script called `generate_data.py`.
This script populates our five main tables (customers, products, product_variants, orders, and order_items) with realistic, time-distributed data that mimics an actual e-commerce platform.

To make this possible, we rely on two lightweight Python libraries:

```diff
+ faker==37     # For realistic names, emails, and categorical data
+ psycopg==3    # For direct PostgreSQL database access
```

These are automatically handled when running with uv. Here's the script:

::code-collapse

```python [generate_data.py]
#!/usr/bin/env python3
"""
E-commerce demo data generator (Postgres).

Usage:
    uv run generate-data --dsn postgresql://user:pass@host:5432/db
"""

import argparse, random
from datetime import datetime, timedelta, timezone
from faker import Faker
import psycopg

# -------------------------------------------------------------------
# GLOBAL CONFIGURATION
# -------------------------------------------------------------------

fake = Faker()

# --- Static categorical constants ---
PRODUCT_CATEGORIES = ["t-shirts", "hoodies", "shoes", "accessories", "jackets"]
COLORS = ["Red", "Blue", "Black", "White", "Green"]
SIZES_APPAREL = ["S", "M", "L", "XL"]
SIZES_SHOES = [36, 38, 40, 42, 44]
ORDER_STATUSES = ["pending", "paid", "cancelled", "shipped"]
ORDER_STATUS_WEIGHTS = [0.2, 0.5, 0.1, 0.2]

# --- SQL Statements ---
SQL_INSERT_CUSTOMERS = """
                       INSERT INTO customers(full_name, email, country, age, created_at, updated_at)
                       VALUES (%s, %s, %s, %s, %s, %s) ON CONFLICT (email) DO
                       UPDATE
                           SET full_name=EXCLUDED.full_name, country=EXCLUDED.country,
                           age=EXCLUDED.age, updated_at=EXCLUDED.updated_at; \
                       """

SQL_INSERT_PRODUCTS = """
                      INSERT INTO products(name, category, sku, created_at, updated_at)
                      VALUES (%s, %s, %s, %s, %s) ON CONFLICT (sku) DO
                      UPDATE
                          SET name=EXCLUDED.name, category=EXCLUDED.category,
                          updated_at=EXCLUDED.updated_at; \
                      """

SQL_INSERT_VARIANTS = """
                      INSERT INTO product_variants
                      (product_id, variant_sku, color, size, manufacturing_price, selling_price,
                       stock_quantity, is_active, created_at, updated_at)
                      VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s) ON CONFLICT (variant_sku) DO
                      UPDATE
                          SET color=EXCLUDED.color, size =EXCLUDED.size,
                          manufacturing_price=EXCLUDED.manufacturing_price,
                          selling_price=EXCLUDED.selling_price,
                          stock_quantity=EXCLUDED.stock_quantity,
                          updated_at=EXCLUDED.updated_at; \
                      """

SQL_DELETE_ORDERS_IN_WINDOW = "DELETE FROM orders WHERE order_date BETWEEN %s AND %s;"

SQL_INSERT_ORDERS = """
                    INSERT INTO orders(customer_id, order_date, status, total_amount, created_at, updated_at)
                    VALUES (%s, %s, %s, %s, %s, %s); \
                    """

SQL_INSERT_ORDER_ITEMS = """
                         INSERT INTO order_items(order_id, variant_id, quantity, unit_price, created_at)
                         VALUES (%s, %s, %s, %s, %s); \
                         """

SQL_UPDATE_ORDER_TOTALS = """
                          UPDATE orders o
                          SET total_amount = COALESCE(oi.sum_total, 0) FROM (
  SELECT order_id, SUM(total_price) sum_total
  FROM order_items GROUP BY order_id
) oi
                          WHERE o.id = oi.order_id
                            AND o.order_date BETWEEN %s
                            AND %s; \
                          """

SQL_STATS_COUNTS = {
    "customers": "SELECT COUNT(*) FROM customers",
    "products": "SELECT COUNT(*) FROM products",
    "variants": "SELECT COUNT(*) FROM product_variants",
    "orders": "SELECT COUNT(*) FROM orders WHERE order_date BETWEEN %s AND %s",
    "items": """
             SELECT COUNT(*)
             FROM order_items oi
                      JOIN orders o ON o.id = oi.order_id
             WHERE o.order_date BETWEEN %s AND %s;
             """,
}

SQL_CHAOS_VALIDATION = """
                       SELECT p.category,
                              SUM(CASE
                                      WHEN (p.category = 'shoes' AND v.size !~ '^[0-9]+$')
                                          OR (p.category <> 'shoes' AND v.size NOT IN ('S', 'M', 'L', 'XL'))
                                          THEN 1
                                      ELSE 0 END) invalid
                       FROM product_variants v
                                JOIN products p ON p.id = v.product_id
                       GROUP BY 1
                       ORDER BY 1; \
                       """


def parse_args():
    now = datetime.now(timezone.utc)
    day_start = now.replace(hour=0, minute=0, second=0, microsecond=0)
    day_end = now.replace(hour=23, minute=59, second=59, microsecond=999999)

    p = argparse.ArgumentParser(description="Generate demo ecommerce data (idempotent).")
    p.add_argument("--dsn", required=True, help="PostgreSQL DSN")
    p.add_argument("--customers", type=int, default=100)
    p.add_argument("--products", type=int, default=50)
    p.add_argument("--orders", type=int, default=500)
    p.add_argument("--max-items-per-order", type=int, default=5)
    p.add_argument("--chaos-percent", type=float, default=0.0, help="%% of variants with invalid sizes")
    p.add_argument("--scale", type=int, default=1, help="Multiply base volumes")
    p.add_argument("--seed", type=int, default=42, help="Deterministic RNG seed")
    p.add_argument("--starting-at", type=str, help="ISO start (e.g. 2024-01-01)")
    p.add_argument("--ending-at", type=str, help="ISO end (e.g. 2024-06-30)")

    a = p.parse_args()
    a.starting_at = a.starting_at or day_start.isoformat()
    a.ending_at = a.ending_at or day_end.isoformat()
    return a


def iso(dt):
    return datetime.fromisoformat(dt).astimezone(timezone.utc)


def rdate(s, e):
    """Return random datetime between s and e."""
    return s + timedelta(seconds=random.randint(0, int((e - s).total_seconds())))


def execmany(conn, sql, rows):
    with conn.cursor() as cur:
        cur.executemany(sql, rows)
    conn.commit()

def gen_customers(n):
    for _ in range(n):
        yield (
            fake.name(),
            fake.unique.email(),
            fake.country(),
            random.randint(18, 70),
            datetime.now(timezone.utc),
            datetime.now(timezone.utc),
        )


def gen_products(n):
    for _ in range(n):
        cat = random.choice(PRODUCT_CATEGORIES)
        yield (
            f"{fake.word().capitalize()} {cat}",
            cat,
            fake.unique.bothify("SKU-####-??").upper(),
            datetime.now(timezone.utc),
            datetime.now(timezone.utc),
        )


def gen_variants(prod_rows, chaos):
    for pid, cat in prod_rows:
        for _ in range(random.randint(1, 4)):
            sku = fake.unique.bothify("VAR-####-??").upper()
            color = random.choice(COLORS)
            size = str(random.choice(SIZES_SHOES)) if cat == "shoes" else random.choice(SIZES_APPAREL)

            # Chaos mode
            if chaos > 0 and random.random() < (chaos / 100):
                if cat == "shoes":
                    size = random.choice(SIZES_APPAREL)
                else:
                    size = str(random.choice(SIZES_SHOES))

            manuf = round(random.uniform(10, 80), 2)
            sell = round(manuf * random.uniform(1.2, 2.0), 2)
            yield (
                pid, sku, color, size, manuf, sell,
                random.randint(0, 200), True,
                datetime.now(timezone.utc), datetime.now(timezone.utc),
            )


def gen_orders(cust_ids, n, s, e):
    for _ in range(n):
        yield (random.choice(cust_ids), rdate(s, e), random.choices(ORDER_STATUSES, ORDER_STATUS_WEIGHTS)[0], 0.0,
               datetime.now(timezone.utc), datetime.now(timezone.utc),)


def gen_items(order_ids, variant_ids, max_items):
    for oid in order_ids:
        for vid in random.sample(variant_ids, random.randint(1, max(1, max_items))):
            yield (oid, vid, random.randint(1, 3), round(random.uniform(20, 200), 2), datetime.now(timezone.utc))


def print_box_summary(S, E, c_cnt, p_cnt, v_cnt, o_cnt, i_cnt, max_items, bad_by_cat):
    print("\n✅ Data generation complete (idempotent window load)")
    print(f"Window: {S.isoformat()} → {E.isoformat()}")
    print(f"Counts — Customers: {c_cnt:,} | Products: {p_cnt:,} | Variants: {v_cnt:,}")
    print(f"Window Facts — Orders: {o_cnt:,} | Items: {i_cnt:,} | MaxItems/Order: {max_items}")
    print("Chaos check (invalid sizes per category):")
    for cat, bad in bad_by_cat:
        print(f"  • {cat}: {bad:,} invalid")


def main():
    args = parse_args()
    random.seed(args.seed)
    Faker.seed(args.seed)
    S, E = iso(args.starting_at), iso(args.ending_at)

    print("\n=== Generating E-commerce Demo Data ===")
    print(f"Window: {S.isoformat()} → {E.isoformat()}")
    print(f"Scale x{args.scale} | Seed {args.seed} | Chaos {args.chaos_percent:.1f}%\n")

    base_c, base_p, base_o = args.customers, args.products, args.orders
    n_c, n_p, n_o = base_c * args.scale, base_p * args.scale, base_o * args.scale

    with psycopg.connect(args.dsn) as conn:
        execmany(conn, SQL_INSERT_CUSTOMERS, gen_customers(n_c))
        execmany(conn, SQL_INSERT_PRODUCTS, gen_products(n_p))

        prod_rows = list(conn.execute("SELECT id, category FROM products"))
        execmany(conn, SQL_INSERT_VARIANTS, gen_variants(prod_rows, args.chaos_percent))

        with conn.cursor() as cur:
            cur.execute(SQL_DELETE_ORDERS_IN_WINDOW, (S, E))
        conn.commit()

        cust_ids = [r[0] for r in conn.execute("SELECT id FROM customers")]
        execmany(conn, SQL_INSERT_ORDERS, gen_orders(cust_ids, n_o, S, E))

        order_ids = [r[0] for r in conn.execute("SELECT id FROM orders WHERE order_date BETWEEN %s AND %s", (S, E))]
        variant_ids = [r[0] for r in conn.execute("SELECT id FROM product_variants")]
        execmany(conn, SQL_INSERT_ORDER_ITEMS, gen_items(order_ids, variant_ids, args.max_items_per_order))

        with conn.cursor() as cur:
            cur.execute(SQL_UPDATE_ORDER_TOTALS, (S, E))
        conn.commit()

        with conn.cursor() as cur:
            cur.execute(SQL_STATS_COUNTS["customers"]);
            c_cnt = cur.fetchone()[0]
            cur.execute(SQL_STATS_COUNTS["products"]);
            p_cnt = cur.fetchone()[0]
            cur.execute(SQL_STATS_COUNTS["variants"]);
            v_cnt = cur.fetchone()[0]
            cur.execute(SQL_STATS_COUNTS["orders"], (S, E));
            o_cnt = cur.fetchone()[0]
            cur.execute(SQL_STATS_COUNTS["items"], (S, E));
            i_cnt = cur.fetchone()[0]
            cur.execute(SQL_CHAOS_VALIDATION);
            bad_by_cat = cur.fetchall()

    print_box_summary(S, E, c_cnt, p_cnt, v_cnt, o_cnt, i_cnt, args.max_items_per_order, bad_by_cat)


if __name__ == "__main__":
    main()
```

::

After that, let's run the script and see if everything works as expected:


::tabs

:::tabs-item{label="Terminal" icon="i-lucide-terminal"}

```bash 
uv run generate_data.py --dsn postgresql://postgres:postgres@localhost:5432/postgres
```

:::

:::tabs-item{label="Terminal (Makefile)" icon="i-lucide-terminal"}

```bash
make generate-data 
```

:::

:::tabs-item{label="Output" icon="i-lucide-eye"}



```bash [Bash Output]
=== Generating E-commerce Demo Data ===
Window: 2025-10-19T00:00:00+00:00 → 2025-10-19T23:59:59.999999+00:00
Scale x1 | Seed 42 | Chaos 0.0%


✅ Data generation complete
Window: 2025-10-19T00:00:00+00:00 → 2025-10-19T23:59:59.999999+00:00
Counts — Customers: 100 | Products: 50 | Variants: 144
Window Facts — Orders: 500 | Items: 1,487 | MaxItems/Order: 5
Chaos check (invalid sizes per category):
  • accessories: 0 invalid
  • hoodies: 0 invalid
  • jackets: 0 invalid
  • shoes: 0 invalid
  • t-shirts: 0 invalid
```

:::

::

Cool! Now we're ready to start our Bruin Pipeline!

<details>

<summary>Click to expand the Script Argument List</summary>

| Argument                | Type             | Default                    | Description                                                                                      |
| ----------------------- | ---------------- | -------------------------- | ------------------------------------------------------------------------------------------------ |
| `--dsn`                 | `str`            | **(required)**             | PostgreSQL connection string (DSN). Example: `postgresql://user:pass@localhost:5432/ecommerce`   |
| `--customers`           | `int`            | `100`                      | Number of customers to generate.                                                                 |
| `--products`            | `int`            | `50`                       | Number of products to generate.                                                                  |
| `--orders`              | `int`            | `500`                      | Number of orders to generate.                                                                    |
| `--max-items-per-order` | `int`            | `5`                        | Maximum number of items per order. Each order will have 1–N items.                               |
| `--chaos-percent`       | `float`          | `0.0`                      | Percentage of product variants that should have **invalid sizes** (used for validation testing). |
| `--starting-at`         | `str (ISO date)` | **Yesterday at 00:00 UTC** | Beginning of the generated data window. Example: `2024-01-01`.                                   |
| `--ending-at`           | `str (ISO date)` | **Today at 23:59 UTC**     | End of the generated data window. Example: `2024-06-30`.                                         |

</details>

> **Note:** You can also run the script with the `--help` flag to see the available arguments.


## 5) Creating the Bruin Pipeline

We have everything structured to start ingesting data into our Bruin Pipeline. So, first, we must create the assets.

But before that, let's understand the structure of the Bruin Pipeline.

A pipeline can be created using the Bruin CLI tool:

```bash [Bruin CLI]
# Example Command
bruin init default some-feature
```

and it will always generate the same structure, where:

* **`.bruin.yml`**
  * The main configuration file for your Bruin environment.
  * Defines **global settings** like default **connections**, variables, and **behavior** for all pipelines.
* **`some-feature/pipeline.yml`**
  * Defines a specific pipeline for a domain or project (in this example, *ecommerce*).
  * Describes the end-to-end data flow — which assets to run, their dependencies, and schedules.
  * Pipelines are modular, so you can maintain separate ones for different business domains.
* **`some-feature/assets/*`**
  * Contains all the assets — the building blocks of your data pipelines.
  * Each asset handles a distinct task: ingesting raw data, transforming it, or generating analytical tables.
  * Since every asset is a file, it’s **version-controlled, testable, and reusable** — just like code.

The important thing to remember is that Bruin will check all files nested inside the `assets` folder for filenames with suffix `.asset.yml`, which can be any type of asset `.asset.*`.

So, doesn't matter how you organize your pipeline assets, by nesting folders. If you follows the convention proposed, Bruin will understand automagically!


## 6. Raw Ingestion Assets

Using `ingestr`, we're able to retrieve data from a source database and ingest it into destination database.

At our case, the goal is to ingest all the data generated from `PostgresSQL` to `DuckDB`, which will act as our analytical database.


Let's take a look at our `assets` and configuration:

:::code-tree{default-value="orders-performance/assets/ingestion/raw.customers.asset.yml"}
```yaml [.bruin.yml]
default_environment: default
environments:
  default:
    connections:
      postgres:
        - name: pg-default
          username: postgres
          password: postgres
          host: localhost
          port: 5432
          database: postgres
      duckdb:
        - name: duckdb-default
          path: duckdb.db

```


```yaml [orders-performance/pipeline.yml]
name: ecommerce_pg_to_duckdb

default_connections:
  duckdb: "duckdb-default"
  postgres: "pg-default"

```

```yaml [orders-performance/assets/ingestion/raw.customers.asset.yml]
name: raw.customers
type: ingestr

description: Ingest OLTP customers from Postgres into DuckDB raw layer.

tags:
  - raw

parameters:
  source_connection: pg-default
  source_table: "public.customers"
  destination: duckdb
```

```yaml [orders-performance/assets/ingestion/raw.orders.asset.yml]
name: raw.orders
type: ingestr
description: Ingest OLTP orders from Postgres into DuckDB raw layer.
tags:
  - raw
parameters:
  destination: duckdb
  source_connection: pg-default
  source_table: public.orders
```

```yaml [orders-performance/assets/ingestion/raw.order_items.asset.yml]
name: raw.order_items
type: ingestr

description: Ingest OdLTP order_items from Postgres into DuckDB raw layer.

tags:
  - raw

depends:
  - raw.orders

parameters:
  source_connection: pg-default
  source_table: "public.order_items"
  destination: duckdb
```

```yaml [orders-performance/assets/ingestion/raw.product_variants.asset.yml]
name: raw.product_variants
type: ingestr
description: Ingest OLTP product_variants from Postgres into DuckDB raw layer.

tags:
  - raw

parameters:
  destination: duckdb
  source_connection: pg-default
  source_table: public.product_variants
```

```yaml [orders-performance/assets/ingestion/raw.products.asset.yml]
name: raw.products
type: ingestr

tags:
  - raw

description: Ingest OLTP products from Postgres into DuckDB raw layer.

parameters:
  source_connection: pg-default
  source_table: public.products
  destination: duckdb
```
:::

The pipeline could run everything concurrently, but we're going to run it in a sequential way by awaiting the dependencies to be processed/resolved.

In Airflow or Dagster, you write DAGs in Python — describing explicitly how tasks depend on one another.
Bruin takes a declarative approach: dependencies are simply listed in YAML under the depends: key, and the system automatically resolves execution order and lineage.

Unlike Airflow (which just schedules tasks) or dbt (which only supports SQL), Bruin allows you to define transformations in both SQL and Python — directly tied to your ingestion assets.

| Pipeline Layer                   | Bruin Equivalent                     |
|----------------------------------|--------------------------------------|
| Ingestion (Airbyte, Fivetran)    | `type: ingestr` assets               |
| Transformation (dbt)             | assets (YML,SQL or Python)           |
| Orchestration (Airflow, Dagster) | Implicit asset graph with `depends:` |


Now, let's see if our ingestion layer is working as expected. Using the Bruin CLI, you must run the following command:

::tabs

:::tabs-item{label="Terminal" icon="i-lucide-terminal"}

```bash 
bruin run orders-performance --tag raw
```

:::



:::tabs-item{label="Output" icon="i-lucide-eye"}



```bash
Analyzed the pipeline 'orders_performance' with 15 assets.

Pipeline: orders_performance (.)
  No issues found

✓ Successfully validated 15 assets across 1 pipeline, all good.

Interval: 2025-10-17T00:00:00Z - 2025-10-17T23:59:59Z
Starting the pipeline execution...

[20:24:23] Running:  raw.products
[20:24:23] Running:  raw.customers
[20:24:23] Running:  raw.orders
[20:24:23] Running:  raw.product_variants

...[logs]...

PASS raw.products 
PASS raw.customers 
PASS raw.orders 
PASS raw.order_items 
PASS raw.product_variants 

bruin run completed successfully in 16.704s

 ✓ Assets executed      5 succeeded
```

:::

::


> Note that using the `--tag` option we'll run only the assets that have the specified tag, avoiding the execution of unnecessary assets.


## 7) Implementing Quality Checks (Staging layer)

The staging layer is where raw data gets **cleaned, validated, and prepared** for analytics. Unlike the raw layer (which mirrors source systems exactly), staging applies business rules, data quality checks, and standardized transformations that make downstream analysis reliable and consistent.

In Bruin, the staging layer combines **SQL transformations** with **declarative quality checks** defined in your `columns` inside an asset.

:::code-tree{default-value="orders-performance/assets/staging/stg.customers.asset.sql"}
```sql [orders-performance/assets/staging/stg.customers.asset.sql]
/* @bruin
name: stg.customers
type: duckdb.sql
materialization:
  type: table
depends:
  - raw.customers
owner: daniel@gmail.com

columns:
  - extends: Customer.ID
  - extends: Customer.Email
  - extends: Customer.Country
  - extends: Customer.Age
  - extends: Base.CreatedAt
  - extends: Base.UpdatedAt
@bruin */

SELECT 
  id::INT AS customer_id, 
  COALESCE(TRIM(email), '') AS email,
  COALESCE(TRIM(country), 'Unknown') AS country,
  COALESCE(age, 0) AS age,
  created_at,
  updated_at
FROM raw.customers
WHERE email IS NOT NULL;
```

```sql [orders-performance/assets/staging/stg.products.sql]
/* @bruin
name: stg.products
type: duckdb.sql
materialization:
  type: table
depends:
  - raw.products

columns:
  - extends: Product.ID
  - extends: Product.Name
  - extends: Product.Category
  - extends: Product.SKU
  - extends: Base.CreatedAt
  - extends: Base.UpdatedAt
@bruin */

SELECT
  id AS product_id,
  name,
  category,
  sku,
  created_at,
  updated_at
FROM raw.products;
```

```sql [orders-performance/assets/staging/stg.product_variants.sql]
/* @bruin
name: stg.product_variants
type: duckdb.sql
materialization:
  type: table
  strategy: create+replace
depends:
  - raw.product_variants
  - stg.products

columns:
  - extends: Variant.ID
  - extends: Variant.ProductID
  - extends: Variant.VariantSKU
  - extends: Variant.Color
  - extends: Variant.Size
  - extends: Variant.ManufacturingPrice
  - extends: Variant.SellingPrice
  - extends: Variant.StockQuantity
  - extends: Variant.IsActive
  - extends: Base.CreatedAt
  - extends: Base.UpdatedAt

custom_checks:
  - name: validate product variant sizes
    description: |
      Ensures that if the product category is 'shoes', the size must be numeric.
      Otherwise, the size must be one of ['S', 'M', 'L'].
    value: 0
    query: |
      SELECT COUNT(*) AS invalid_count
      FROM raw.product_variants v
      JOIN stg.products p ON p.product_id = v.product_id
      WHERE
        (p.category = 'shoes' AND NOT (v.size ~ '^[0-9]+$'))
        OR (p.category != 'shoes' AND v.size NOT IN ('S', 'M', 'L'))
@bruin */

SELECT
  pv.id AS variant_id,
  pv.product_id,
  pv.variant_sku,
  pv.color,
  pv.size,
  CASE WHEN pv.manufacturing_price < 0 THEN 0 ELSE pv.manufacturing_price END AS manufacturing_price,
  CASE WHEN pv.selling_price < 0 THEN 0 ELSE pv.selling_price END AS selling_price,
  CASE WHEN pv.stock_quantity < 0 THEN 0 ELSE pv.stock_quantity END AS stock_quantity,
  COALESCE(pv.is_active, TRUE) AS is_active,
  pv.created_at,
  pv.updated_at
FROM raw.product_variants pv
JOIN stg.products p ON p.product_id = pv.product_id;
```

```sql [orders-performance/assets/staging/stg.orders.sql]
/* @bruin
name: stg.orders
type: duckdb.sql
materialization:
  type: table
  strategy: create+replace
depends:
  - raw.orders
  - raw.customers

columns:
  - extends: Order.ID
  - extends: Order.CustomerID
  - extends: Order.OrderDate
  - extends: Order.Status
  - extends: Order.TotalAmount
@bruin */

WITH src AS (
  SELECT
    o.id              AS order_id,
    o.customer_id,
    o.order_date,
    o.status,
    CASE WHEN o.total_amount < 0 THEN 0 ELSE o.total_amount END AS total_amount
  FROM raw.orders o
)
SELECT * FROM src;
```

```sql [orders-performance/assets/staging/stg.order_items.sql]
/* @bruin
name: stg.order_items
type: duckdb.sql
materialization:
  type: table
depends:
  - raw.order_items

columns:
  - extends: OrderItem.ID
  - extends: OrderItem.OrderID
  - extends: OrderItem.VariantID
  - extends: OrderItem.Quantity
  - extends: OrderItem.UnitPrice
  - extends: OrderItem.TotalPrice
  - extends: Base.CreatedAt
@bruin */

SELECT
  oi.id AS order_item_id,
  oi.order_id,
  oi.variant_id,
  oi.quantity,
  CASE WHEN oi.unit_price < 0 THEN 0 ELSE oi.unit_price END AS unit_price,
  CASE WHEN oi.total_price < 0 THEN 0 ELSE oi.total_price END AS total_price,
  oi.created_at
FROM raw.order_items oi;
```

```yaml [glossary.yml]
domains:
  customer-management:
    description: All aspects related to customer data, registration, and account management
    owners:
      - "Data Team"
    tags:
      - customer
  product-catalog:
    description: Product and catalog data including variants and SKUs
    owners:
      - "Data Team"
    tags:
      - product
      - catalog
  sales:
    description: Orders and order items lifecycle data
    owners:
      - "Data Team"
    tags:
      - sales

entities:
  Base:
    description: Required fieds for every Ingestion/Staging step.
    attributes:
      ID:
        name: id
        type: integer
        primary_key: true
        description: The unique identifier in our systems.
        checks:
          - name: not_null
          - name: unique
      
      CreatedAt:
        name: created_at
        type: timestamp
        description: Timestamp when the record was created.
        checks:
          - name: not_null

      UpdatedAt:
        name: updated_at
        type: timestamp
        description: Timestamp when the record was last updated.
        checks:
          - name: not_null

  Customer:
    description: Customer is an individual/business registered on our platform.
    domains:
      - customer-management
    attributes:
      ID:
        name: customer_id
        type: integer
        description: Surrogate key for customer in staging.
        checks:
          - name: not_null
          - name: unique
      Email:
        name: email
        type: string
        description: E-mail address used during registration.
        checks:
          - name: not_null
      FullName:
        name: full_name
        type: string
        description: Full name of the customer.
      Country:
        name: country
        type: string
        description: Country of residence.
      Age:
        name: age
        type: integer
        description: Age of the customer.
        checks:
          - name: range
            min: 0

  Product:
    description: A product offered for sale.
    domains:
      - product-catalog
    attributes:
      ID:
        name: product_id
        type: integer
        description: Surrogate key for product in staging.
        checks:
          - name: not_null
          - name: unique
      Name:
        name: name
        type: string
        description: Product display name.
        checks:
          - name: not_null
      Category:
        name: category
        type: string
        description: Product category.
        checks:
          - name: not_null
      SKU:
        name: sku
        type: string
        description: Stock keeping unit for the product.
        checks:
          - name: not_null
          - name: unique

  Variant:
    description: A specific variation of a product.
    domains:
      - product-catalog
    attributes:
      ID:
        name: variant_id
        type: integer
        description: Surrogate key for variant in staging.
        checks:
          - name: not_null
          - name: unique
      ProductID:
        name: product_id
        type: integer
        description: Reference to Product.ID.
        checks:
          - name: not_null
          - name: positive
      VariantSKU:
        name: variant_sku
        type: string
        description: SKU for the variant.
        checks:
          - name: not_null
          - name: unique
      Color:
        name: color
        type: string
        description: Variant color.
      Size:
        name: size
        type: string
        description: Variant size.
      ManufacturingPrice:
        name: manufacturing_price
        type: numeric
        description: Manufacturing cost per unit.
        checks:
          - name: not_null
      SellingPrice:
        name: selling_price
        type: numeric
        description: Selling price per unit.
        checks:
          - name: not_null
      StockQuantity:
        name: stock_quantity
        type: integer
        description: Available stock quantity.
        checks:
          - name: not_null
      IsActive:
        name: is_active
        type: boolean
        description: Whether the variant is active.
        checks:
          - name: not_null

  Order:
    description: A purchase order placed by a customer.
    domains:
      - sales
    attributes:
      ID:
        name: order_id
        type: integer
        description: Surrogate key for order in staging.
        checks:
          - name: not_null
          - name: unique
      CustomerID:
        name: customer_id
        type: integer
        description: Reference to Customer.ID for the purchaser.
        checks:
          - name: not_null
          - name: positive
      OrderNumber:
        name: order_number
        type: string
        description: Human-readable order number.
      OrderDate:
        name: order_date
        type: timestamp
        description: When the order was placed.
        checks:
          - name: not_null
      Status:
        name: status
        type: string
        description: Order lifecycle status.
        checks:
          - name: not_null
          - name: accepted_values
            value:
              - pending
              - paid
              - cancelled
              - shipped
      TotalAmount:
        name: total_amount
        type: numeric
        description: Total order amount.
        checks:
          - name: not_null
          - name: range
            min: 0

  OrderItem:
    description: A line item in an order.
    domains:
      - sales
    attributes:
      ID:
        name: order_item_id
        type: integer
        description: Surrogate key for order item in staging.
        checks:
          - name: not_null
          - name: unique
      OrderID:
        name: order_id
        type: integer
        description: Reference to Order.ID this item belongs to.
        checks:
          - name: not_null
          - name: positive
      VariantID:
        name: variant_id
        type: integer
        description: Reference to Variant.ID purchased.
        checks:
          - name: not_null
          - name: positive
      Quantity:
        name: quantity
        type: integer
        description: Quantity purchased (must be positive).
        checks:
          - name: not_null
          - name: range
            min: 1
      UnitPrice:
        name: unit_price
        type: numeric
        description: Price per unit at purchase time.
        checks:
          - name: not_null
          - name: range
            min: 0
      TotalPrice:
        name: total_price
        type: numeric
        description: Total price for the line (Quantity * UnitPrice).
        checks:
          - name: not_null
          - name: range
            min: 0

```
:::


You can work with column check by adding inside your asset the `columns` the needed definitions for the output of the column that you will materialize:

```sql {12-18} [orders-performance/assets/staging/stg.customers.asset.sql]
/* @bruin
name: stg.customers
type: duckdb.sql

materialization:
  type: table

depends:
  - raw.customers
owner: daniel@gmail.com

columns: 
  - name: customer_id 
    type: integer
    primary_key: true
    checks:
      - name: unique
      - name: not_null
@bruin */

SELECT 
  id::INT AS customer_id, 
  COALESCE(TRIM(email), '') AS email,
  COALESCE(TRIM(country), 'Unknown') AS country,
  COALESCE(age, 0) AS age,
  created_at,
  updated_at
FROM raw.customers
WHERE email IS NOT NULL;
```

**Available Check Types**

| Check Type        | Purpose                          | Example Configuration                                                      |
| ----------------- | -------------------------------- | -------------------------------------------------------------------------- |
| `not_null`        | Ensures no NULL values           | `- name: not_null`                                                         |
| `unique`          | Ensures no duplicate values      | `- name: unique`                                                           |
| `range`           | Validates numeric ranges         | `- name: range`<br>`  min: 0`<br>`  max: 100`                              |
| `accepted_values` | Validates against allowed values | `- name: accepted_values`<br>`  value:`<br>`    - pending`<br>`    - paid` |
| `positive`        | Ensures positive numbers         | `- name: positive`                                                         |
| `regex`           | Pattern matching validation      | `- name: regex`<br>`  pattern: '^[A-Z]{3}-\d{4}$'`                         |



Bruin's approach to data quality is **declarative and reusable**. Instead of writing custom validation SQL for every table, you define **entities and attributes** in `glossary.yml`, then reference them in your staging assets using the `extends:` keyword.

Here's how the column check system works:

**Entity Definition in glossary.yml:**

```yaml
entities:
  Customer:
    description: Customer is an individual/business registered on our platform.
    domains:
      - customer-management
    attributes:
      Email:
        name: email
        type: string
        description: E-mail address used during registration.
        checks:
          - name: not_null
      Age:
        name: age
        type: integer
        description: Age of the customer.
        checks:
          - name: range
            min: 0
```

Then, at your asset you could easily just extends it, avoiding duplicate validations across other assets:

```sql
/* @bruin
columns:
  - extends: Customer.Email    # Inherits not_null check
  - extends: Customer.Age      # Inherits range check (min: 0)
@bruin */
```

For complex validations that go beyond column-level checks, Bruin supports **custom checks** with SQL queries:

```yaml
custom_checks:
  - name: validate product variant sizes
    description: |
      Ensures that if the product category is 'shoes', the size must be numeric.
      Otherwise, the size must be one of ['S', 'M', 'L'].
    value: 0  # Expected result (0 = no violations)
    query: |
      SELECT COUNT(*) AS invalid_count
      FROM raw.product_variants v
      JOIN stg.products p ON p.product_id = v.product_id
      WHERE
        (p.category = 'shoes' AND NOT (v.size ~ '^[0-9]+$'))
        OR (p.category != 'shoes' AND v.size NOT IN ('S', 'M', 'L'))
```


Now that we understood the basics of Bruin's data quality system, let's run the staging pipeline:



::tabs

:::tabs-item{label="Terminal" icon="i-lucide-terminal"}

```bash 
bruin run orders-performance --tag staging
```

:::

:::tabs-item{label="Output" icon="i-lucide-eye"}


::code-collapse
```bash

Analyzed the pipeline 'orders_performance' with 15 assets.

Pipeline: orders_performance (.)
  No issues found

✓ Successfully validated 15 assets across 1 pipeline, all good.

Interval: 2025-10-17T00:00:00Z - 2025-10-17T23:59:59Z

Starting the pipeline execution...

[20:59:31] Running:  stg.products
[20:59:31] Running:  stg.order_items
[20:59:31] Running:  stg.orders
[20:59:31] Running:  stg.customers
[20:59:31] Finished: stg.products (28ms)
[20:59:31] Running:  stg.product_variants
[20:59:31] Finished: stg.orders (57ms)
[20:59:31] Finished: stg.order_items (78ms)
[20:59:31] Finished: stg.customers (101ms)
[20:59:31] Finished: stg.product_variants (89ms)
[20:59:31] Running:  stg.product_variants:custom-check:validate_product_variant_sizes
[20:59:31] Finished: stg.product_variants:custom-check:validate_product_variant_sizes (6ms)

==================================================

PASS stg.products 
PASS stg.orders 
PASS stg.order_items 
PASS stg.customers 
PASS stg.product_variants .


bruin run completed successfully in 125ms

 ✓ Assets executed      5 succeeded
 ✓ Quality checks       1 succeeded

```

::

:::

::



Each asset runs its quality checks alongside transformations, keeping data accurate at every step. You can reuse validation rules across assets, add business context through glossaries, and rely on incremental checks that catch issues early.


## 8) Designing and Running Your Analytics Layer

Here we define models with a clear grain, apply final business rules, and expose KPIs for decision-making. These models typically power
dashboards and self-service exploration.

Just like in the Raw and Staging sections above, below is a code tree that shows how the mart is structured in our
orders-performance pipeline. Each model consumes stg.* tables and produces clean outputs.

::code-tree{default-value="mart.sales_daily.asset.sql"}
```sql [mart.sales_daily.asset.sql]
/* @bruin

name: mart.sales_daily
type: duckdb.sql

materialization:
  type: table
  strategy: append

depends:
  - stg.order_items
  - stg.orders

columns:
  - name: sale_date
    type: date
    checks:
      - name: not_null
  - name: orders_count
    type: integer
    checks:
      - name: not_null
  - name: items_count
    type: integer
    checks:
      - name: not_null
  - name: revenue
    type: numeric
    checks:
      - name: not_null

custom_checks:
  - name: total revenue non-negative
    value: 0
    query: SELECT COUNT(*) FROM mart.sales_daily WHERE revenue < 0

@bruin */

WITH daily_orders AS (SELECT CAST(CAST(order_date AS TIMESTAMP) AS DATE)                               AS sale_date,
                             COUNT(*)                                                                  AS orders_count,
                             SUM(CASE WHEN status IN ('paid', 'shipped') THEN total_amount ELSE 0 END) AS revenue
                      FROM stg.orders
                      GROUP BY 1),
     daily_items AS (SELECT CAST(CAST(o.order_date AS TIMESTAMP) AS DATE) AS sale_date,
                            COUNT(*)                                      AS items_count
                     FROM stg.order_items oi
                            JOIN stg.orders o ON o.order_id = oi.order_id
                     GROUP BY 1)
SELECT d.sale_date,
       d.orders_count,
       COALESCE(i.items_count, 0) AS items_count,
       d.revenue
FROM daily_orders d
ORDER BY sale_date;
```

```sql [mart.product_performance.asset.sql]
/* @bruin

name: mart.product_performance
type: duckdb.sql

materialization:
  type: table
  strategy: create+replace

depends:
  - stg.products
  - stg.product_variants
  - stg.order_items
  - stg.orders

columns:
  - name: product_id
    type: BIGINT
    checks:
      - name: not_null
  - name: product_name
    type: VARCHAR
    checks:
      - name: not_null
  - name: category
    type: VARCHAR
    checks:
      - name: not_null
  - name: items_sold
    type: HUGEINT
    checks:
      - name: not_null
  - name: gross_revenue
    type: DECIMAL(38,2)
    checks:
      - name: not_null
  - name: avg_item_price
    type: DOUBLE
    checks:
      - name: not_null

custom_checks:
  - name: no negative prices or revenue
    value: 0
    query: SELECT COUNT(*) FROM mart.product_performance WHERE avg_item_price < 0 OR gross_revenue < 0

@bruin */

WITH paid_items AS (
  SELECT oi.*
  FROM stg.order_items oi
  JOIN stg.orders o ON o.order_id = oi.order_id
  WHERE o.status IN ('paid','shipped')
), items_with_product AS (
  SELECT
    v.product_id,
    oi.quantity,
    oi.total_price
  FROM paid_items oi
  JOIN stg.product_variants v ON v.variant_id = oi.variant_id
)
SELECT
  p.product_id,
  p.name AS product_name,
  p.category,
  SUM(i.quantity) AS items_sold,
  SUM(i.total_price) AS gross_revenue,
  CASE WHEN SUM(i.quantity) = 0 THEN 0 ELSE SUM(i.total_price) / SUM(i.quantity) END AS avg_item_price
FROM items_with_product i
JOIN stg.products p ON p.product_id = i.product_id
GROUP BY 1,2,3
ORDER BY gross_revenue DESC;
```

```sql [mart.variant_profitability.asset.sql]
/* @bruin

name: mart.variant_profitability
type: duckdb.sql

materialization:
  type: table
  strategy: create+replace

depends:
  - stg.product_variants
  - stg.order_items
  - stg.orders
  - stg.products

columns:
  - name: variant_id
    type: integer
    checks:
      - name: not_null
      - name: unique
  - name: variant_sku
    type: string
    checks:
      - name: not_null
  - name: product_id
    type: integer
    checks:
      - name: not_null
  - name: items_sold
    type: integer
    checks:
      - name: not_null
  - name: revenue
    type: numeric
    checks:
      - name: not_null
  - name: cost
    type: numeric
    checks:
      - name: not_null
  - name: profit
    type: numeric
    checks:
      - name: not_null
  - name: margin_pct
    type: numeric
    checks:
      - name: not_null

@bruin */

WITH paid_items AS (
  SELECT oi.*
  FROM stg.order_items oi
  JOIN stg.orders o ON o.order_id = oi.order_id
  WHERE o.status IN ('paid','shipped')
), joined AS (
  SELECT
    v.variant_id,
    p.name as product_name,
    v.variant_sku,
    v.product_id,
    v.manufacturing_price,
    v.selling_price,
    oi.quantity,
    oi.total_price
  FROM paid_items oi
  JOIN stg.product_variants v ON v.variant_id = oi.variant_id
  JOIN stg.products p ON p.product_id = v.product_id
)
SELECT
    j.product_name,
  j.variant_id,
  j.variant_sku,
  j.product_id,
  SUM(j.quantity) AS items_sold,
  SUM(j.total_price) AS revenue,
  SUM(j.quantity * j.manufacturing_price) AS cost,
  SUM(j.total_price) - SUM(j.quantity * j.manufacturing_price) AS profit,
  CASE WHEN SUM(j.total_price) = 0 THEN 0
       ELSE (SUM(j.total_price) - SUM(j.quantity * j.manufacturing_price)) / SUM(j.total_price)
  END AS margin_pct,
  BOOL_OR(j.selling_price < j.manufacturing_price) AS selling_price_below_cost
FROM joined j
GROUP BY 1,2,3,4
ORDER BY profit DESC;
```


```sql [mart.customers_by_age.asset.sql]
/* @bruin

name: mart.customers_by_age
type: duckdb.sql
tags:
  - mart
  - amazing-tag

materialization:
  type: table
  strategy: create+replace

depends:
  - stg.customers
owner: danielhe4rt@gmail.com

@bruin */

WITH src AS (SELECT CASE
                        WHEN age < 25 THEN '18-24'
                        WHEN age BETWEEN 25 AND 34 THEN '25-34'
                        WHEN age BETWEEN 35 AND 49 THEN '35-49'
                        ELSE '50+'
                        END AS age_group
             FROM stg.customers)
SELECT age_group,
       COUNT(*) AS total_customers
FROM src
GROUP BY age_group
ORDER BY total_customers DESC;
```

```sql [mart.customers_by_county.asset.sql]
/* @bruin

name: mart.customers_by_country
type: duckdb.sql

materialization:
  type: table
  strategy: create+replace

depends:
  - stg.customers

@bruin */

SELECT
    country,
    COUNT(*) AS total_customers
  FROM stg.customers
  GROUP BY country
  ORDER BY total_customers DESC
```

::


So now, we clearly have a data pipeline that turns raw data into well-structured, curated datasets which can be used
for analysis. We can also leverage the power of Bruin's data quality system to ensure that our data is always up-to-date
and accurate.

Let's run the mart pipeline:

::tabs

:::tabs-item{label="Terminal" icon="i-lucide-terminal"}

```bash 
bruin run orders-performance --tag mart 
```

:::

:::tabs-item{label="Output" icon="i-lucide-eye"}


::code-collapse

```bash
bruin run orders-performance --tag mart 
Analyzed the pipeline 'orders_performance' with 15 assets.

Pipeline: orders_performance (.)
  No issues found                                                                                                                                                                   

✓ Successfully validated 15 assets across 1 pipeline, all good.                                                                                                                     
                                                                                                                                                                                    
Interval: 2025-10-18T00:00:00Z - 2025-10-18T23:59:59Z

Starting the pipeline execution...

[23:23:17] Running:  mart.sales_daily
[23:23:17] Running:  mart.customers_by_country
[23:23:17] Running:  mart.product_performance
[23:23:17] Running:  mart.variant_profitability
[23:23:17] Running:  mart.customers_by_age
[23:23:17] Finished: mart.sales_daily (31ms)
[23:23:17] Running:  mart.sales_daily:custom-check:total_revenue_non-negative
[23:23:17] Running:  mart.sales_daily:revenue:not_null
[23:23:17] Running:  mart.sales_daily:sale_date:not_null
[23:23:17] Running:  mart.sales_daily:orders_count:not_null
[23:23:17] Running:  mart.sales_daily:items_count:not_null
[23:23:17] Finished: mart.customers_by_country (55ms)
[23:23:17] Finished: mart.sales_daily:custom-check:total_revenue_non-negative (28ms)
[23:23:17] Finished: mart.sales_daily:revenue:not_null (34ms)
[23:23:17] Finished: mart.customers_by_age (93ms)
[23:23:17] Finished: mart.sales_daily:sale_date:not_null (69ms)
[23:23:17] Finished: mart.variant_profitability (127ms)
[23:23:17] Running:  mart.variant_profitability:margin_pct:not_null
[23:23:17] Running:  mart.variant_profitability:product_id:not_null
[23:23:17] Running:  mart.variant_profitability:cost:not_null
[23:23:17] Running:  mart.variant_profitability:profit:not_null
[23:23:17] Running:  mart.variant_profitability:variant_sku:not_null
[23:23:17] Running:  mart.variant_profitability:revenue:not_null
[23:23:17] Running:  mart.variant_profitability:items_sold:not_null
[23:23:17] Running:  mart.variant_profitability:variant_id:unique
[23:23:17] Running:  mart.variant_profitability:variant_id:not_null
[23:23:17] Finished: mart.variant_profitability:margin_pct:not_null (5ms)
[23:23:17] Finished: mart.variant_profitability:profit:not_null (11ms)
[23:23:17] Finished: mart.variant_profitability:cost:not_null (17ms)
[23:23:17] Finished: mart.variant_profitability:variant_id:unique (23ms)
[23:23:17] Finished: mart.variant_profitability:variant_id:not_null (30ms)
[23:23:17] Finished: mart.variant_profitability:revenue:not_null (36ms)
[23:23:17] Finished: mart.variant_profitability:product_id:not_null (42ms)
[23:23:17] Finished: mart.variant_profitability:variant_sku:not_null (49ms)
[23:23:17] Finished: mart.variant_profitability:items_sold:not_null (55ms)
[23:23:17] Finished: mart.sales_daily:items_count:not_null (161ms)
[23:23:17] Finished: mart.sales_daily:orders_count:not_null (168ms)
[23:23:17] Finished: mart.product_performance (238ms)
[23:23:17] Running:  mart.product_performance:custom-check:no_negative_prices_or_revenue
[23:23:17] Running:  mart.product_performance:category:not_null
[23:23:17] Running:  mart.product_performance:avg_item_price:not_null
[23:23:17] Running:  mart.product_performance:product_id:not_null
[23:23:17] Running:  mart.product_performance:gross_revenue:not_null
[23:23:17] Running:  mart.product_performance:product_name:not_null
[23:23:17] Running:  mart.product_performance:items_sold:not_null
[23:23:17] Finished: mart.product_performance:category:not_null (5ms)
[23:23:17] Finished: mart.product_performance:custom-check:no_negative_prices_or_revenue (11ms)
[23:23:17] Finished: mart.product_performance:product_id:not_null (18ms)
[23:23:17] Finished: mart.product_performance:product_name:not_null (23ms)
[23:23:17] Finished: mart.product_performance:gross_revenue:not_null (29ms)
[23:23:17] Finished: mart.product_performance:avg_item_price:not_null (35ms)
[23:23:17] Finished: mart.product_performance:items_sold:not_null (43ms)

==================================================

PASS mart.sales_daily .....
PASS mart.customers_by_country 
PASS mart.customers_by_age 
PASS mart.variant_profitability .........
PASS mart.product_performance .......


bruin run completed successfully in 281ms

 ✓ Assets executed      5 succeeded
 ✓ Quality checks       21 succeeded

```

::

:::

::

Everything worked as expected! Now we can start using/querying our data using the CLI:

::tabs

:::tabs-item{label="Terminal" icon="i-lucide-terminal"}

```bash 
for t in \
  mart.sales_daily \
  mart.product_performance \
  mart.variant_profitability \
  mart.customers_by_age \
  mart.customers_by_country; do
  bruin query --c duckdb-default --q "select * from $t limit 10"
done

```

:::

:::tabs-item{label="Terminal (Makefile)" icon="i-lucide-file-terminal"}

```bash 
make mart-list
```

:::

:::tabs-item{label="Output" icon="i-lucide-eye"}

::code-collapse

```bash
┌───────────────────────────────┬──────────────┬─────────────┬───────────┐
│ SALE_DATE                     │ ORDERS_COUNT │ ITEMS_COUNT │ REVENUE   │
├───────────────────────────────┼──────────────┼─────────────┼───────────┤
│ 2025-10-20 00:00:00 +0000 UTC │ 440          │ 1328        │ 206063.72 │
│ 2025-10-21 00:00:00 +0000 UTC │ 60           │ 178         │ 26792.41  │
└───────────────────────────────┴──────────────┴─────────────┴───────────┘
┌────────────┬────────────────────┬─────────────┬────────────┬───────────────┬────────────────────┐
│ PRODUCT_ID │ PRODUCT_NAME       │ CATEGORY    │ ITEMS_SOLD │ GROSS_REVENUE │ AVG_ITEM_PRICE     │
├────────────┼────────────────────┼─────────────┼────────────┼───────────────┼────────────────────┤
│ 26         │ Well hoodies       │ hoodies     │ 101        │ 11578.84      │ 114.64198019801981 │
│ 8          │ Often shoes        │ shoes       │ 84         │ 9511.24       │ 113.22904761904762 │
│ 9          │ Professor t-shirts │ t-shirts    │ 82         │ 9300.89       │ 113.42548780487805 │
│ 41         │ White t-shirts     │ t-shirts    │ 80         │ 9202.42       │ 115.03025          │
│ 30         │ Pull jackets       │ jackets     │ 80         │ 8586.61       │ 107.33262500000001 │
│ 37         │ Resource jackets   │ jackets     │ 74         │ 8289.51       │ 112.02040540540541 │
│ 35         │ Others hoodies     │ hoodies     │ 73         │ 7810.05       │ 106.98698630136987 │
│ 10         │ Old hoodies        │ hoodies     │ 66         │ 7365.61       │ 111.60015151515151 │
│ 16         │ Nothing hoodies    │ hoodies     │ 61         │ 7222.53       │ 118.40213114754098 │
│ 38         │ Along accessories  │ accessories │ 54         │ 6740.6        │ 124.82592592592593 │
└────────────┴────────────────────┴─────────────┴────────────┴───────────────┴────────────────────┘
┌────────────────────┬────────────┬─────────────┬────────────┬────────────┬─────────┬────────┬─────────┬────────────────────┬──────────────────────────┐
│ PRODUCT_NAME       │ VARIANT_ID │ VARIANT_SKU │ PRODUCT_ID │ ITEMS_SOLD │ REVENUE │ COST   │ PROFIT  │ MARGIN_PCT         │ SELLING_PRICE_BELOW_COST │
├────────────────────┼────────────┼─────────────┼────────────┼────────────┼─────────┼────────┼─────────┼────────────────────┼──────────────────────────┤
│ White t-shirts     │ 111        │ VAR-7584-ZF │ 41         │ 35         │ 5118.96 │ 1022   │ 4096.96 │ 0.8003500711081939 │ false                    │
│ House jackets      │ 75         │ VAR-2214-YG │ 27         │ 19         │ 3119.34 │ 390.64 │ 2728.7  │ 0.8747683804907448 │ false                    │
│ East shoes         │ 50         │ VAR-1369-ZH │ 18         │ 22         │ 3051.23 │ 563.2  │ 2488.03 │ 0.8154186999996723 │ false                    │
│ Well hoodies       │ 72         │ VAR-5281-PQ │ 26         │ 29         │ 3192.99 │ 816.06 │ 2376.93 │ 0.7444213730703823 │ false                    │
│ Town hoodies       │ 63         │ VAR-7820-CG │ 23         │ 26         │ 3068.31 │ 776.62 │ 2291.69 │ 0.7468899817815019 │ false                    │
│ Prepare shoes      │ 65         │ VAR-7788-NK │ 24         │ 21         │ 2700.78 │ 426.51 │ 2274.27 │ 0.8420789549685646 │ false                    │
│ Indicate hoodies   │ 114        │ VAR-4479-TB │ 43         │ 20         │ 2623.64 │ 359.8  │ 2263.84 │ 0.8628622829351589 │ false                    │
│ Professor t-shirts │ 24         │ VAR-8098-JV │ 9          │ 30         │ 3144.22 │ 899.1  │ 2245.12 │ 0.7140467270101964 │ false                    │
│ Under t-shirts     │ 109        │ VAR-4156-ZV │ 40         │ 20         │ 2673.75 │ 503    │ 2170.75 │ 0.8118747078073866 │ false                    │
│ Others hoodies     │ 91         │ VAR-7790-FB │ 35         │ 27         │ 2555.88 │ 413.64 │ 2142.24 │ 0.8381614160289215 │ false                    │
└────────────────────┴────────────┴─────────────┴────────────┴────────────┴─────────┴────────┴─────────┴────────────────────┴──────────────────────────┘
┌───────────┬─────────────────┐
│ AGE_GROUP │ TOTAL_CUSTOMERS │
├───────────┼─────────────────┤
│ 50+       │ 39              │
│ 35-49     │ 24              │
│ 25-34     │ 19              │
│ 18-24     │ 18              │
└───────────┴─────────────────┘
┌──────────────────────────────────────────────┬─────────────────┐
│ COUNTRY                                      │ TOTAL_CUSTOMERS │
├──────────────────────────────────────────────┼─────────────────┤
│ Chad                                         │ 3               │
│ Cayman Islands                               │ 3               │
│ Lithuania                                    │ 2               │
│ Libyan Arab Jamahiriya                       │ 2               │
│ South Georgia and the South Sandwich Islands │ 2               │
│ Korea                                        │ 2               │
│ El Salvador                                  │ 2               │
│ Guam                                         │ 2               │
│ Egypt                                        │ 2               │
│ Denmark                                      │ 2               │
└──────────────────────────────────────────────┴─────────────────┘
```

::

:::

::


## 9) Introducing Chaos Data

Now that you can generate deterministic data for your local pipeline, let’s add **controlled chaos** — a small but powerful feature to validate your **data quality checks** in Bruin. But why chaos?

- In real-world scenarios, upstream systems sometimes send **invalid or inconsistent data**.
- For instance, a product categorized as `shoes` might contain an invalid `size` value like `"M"` instead of `"42"`.
- We intentionally simulate these cases so you can **test your validation logic** during ingestion.


The `generate_data.py` script includes a `--chaos-percent` flag that controls how many product variants should be randomly invalidated.


::tabs

:::tabs-item{label="Terminal" icon="i-lucide-terminal"}

```bash 
uv run generate-data --dsn postgresql://postgres:postgres@localhost:5432/postgres --chaos-percent 5
```

:::

:::tabs-item{label="Terminal (Makefile)" icon="i-lucide-file-terminal"}

```bash 
make generate-chaos SCALE=1 CHAOS=1
```

:::

:::tabs-item{label="Output" icon="i-lucide-eye"}

```bash
=== Generating E-commerce Demo Data ===
Window: 2025-10-20T00:00:00+00:00 → 2025-10-20T23:59:59.999999+00:00
Scale x1 | Seed 42 | Chaos 5.0%

✅ Data generation complete (idempotent window load)
Window: 2025-10-20T00:00:00+00:00 → 2025-10-20T23:59:59.999999+00:00
Counts — Customers: 100 | Products: 50 | Variants: 127
Window Facts — Orders: 500 | Items: 1,495 | MaxItems/Order: 5
Chaos check (invalid sizes per category):
  • accessories: 2 invalid
  • hoodies: 4 invalid
  • jackets: 4 invalid
  • shoes: 19 invalid
  • t-shirts: 2 invalid
```

:::

::


This command generates **5% of product variants** with inconsistent sizes:

* Products in the `shoes` category might get alphabetic sizes (`S`, `M`, `L`) instead of numeric ones.
* Non-shoe products (`t-shirts`, `hoodies`, etc.) might receive numeric sizes like `37` or `41`.

You can use any chaos level (e.g., `--chaos-percent 10`), or leave it at the default (`0.0`) for clean datasets.

Bruin validation assets can now reference this rule.
Here’s the SQL query that checks for these inconsistencies in your **`stg.product_variants`** layer:

::code-collapse

```sql [orders-performance/assets/staging/stg.product_variants.sql] {24-36}
/* @bruin
name: stg.product_variants
type: duckdb.sql
materialization:
  type: table
  strategy: create+replace
depends:
  - raw.product_variants
  - stg.products

columns:
  - extends: Variant.ID
  - extends: Variant.ProductID
  - extends: Variant.VariantSKU
  - extends: Variant.Color
  - extends: Variant.Size
  - extends: Variant.ManufacturingPrice
  - extends: Variant.SellingPrice
  - extends: Variant.StockQuantity
  - extends: Variant.IsActive
  - extends: Base.CreatedAt
  - extends: Base.UpdatedAt

custom_checks:
  - name: validate product variant sizes
    description: |
      Ensures that if the product category is 'shoes', the size must be numeric.
      Otherwise, the size must be one of ['S', 'M', 'L'].
    value: 0
    query: |
      SELECT COUNT(*) AS invalid_count
      FROM raw.product_variants v
      JOIN stg.products p ON p.product_id = v.product_id
      WHERE
        (p.category = 'shoes' AND NOT (v.size ~ '^[0-9]+$'))
        OR (p.category != 'shoes' AND v.size NOT IN ('S', 'M', 'L'))
@bruin */

SELECT
  pv.id AS variant_id,
  pv.product_id,
  pv.variant_sku,
  pv.color,
  pv.size,
  CASE WHEN pv.manufacturing_price < 0 THEN 0 ELSE pv.manufacturing_price END AS manufacturing_price,
  CASE WHEN pv.selling_price < 0 THEN 0 ELSE pv.selling_price END AS selling_price,
  CASE WHEN pv.stock_quantity < 0 THEN 0 ELSE pv.stock_quantity END AS stock_quantity,
  COALESCE(pv.is_active, TRUE) AS is_active,
  pv.created_at,
  pv.updated_at
FROM raw.product_variants pv
JOIN stg.products p ON p.product_id = pv.product_id;
```

::

If you introduced 5% chaos, this query should return a small number of invalid records — just enough to confirm your validation layer is working as expected. Let's run the full pipeline and see if the validation layer catches any errors:

::tabs

:::tabs-item{label="Terminal" icon="i-lucide-terminal"}

```bash 
bruin run orders-performance
```

:::

:::tabs-item{label="Output" icon="i-lucide-eye"}


::code-collapse

```bash
bruin run orders-performance
Analyzed the pipeline 'orders_performance' with 15 assets.

Pipeline: orders_performance (.)
  No issues found                                                                                                                                                                   

✓ Successfully validated 15 assets across 1 pipeline, all good.                                                                                                                     
                                                                                                                                                                                    
Interval: 2025-10-18T00:00:00Z - 2025-10-18T23:59:59Z

Starting the pipeline execution...

...

PASS raw.products 
PASS stg.products 
PASS raw.customers 
PASS stg.customers 
PASS mart.customers_by_age 
PASS mart.customers_by_country 
PASS raw.product_variants 
PASS stg.product_variants F
PASS raw.orders 
PASS stg.orders 
PASS raw.order_items 
PASS stg.order_items 
PASS mart.sales_daily .....
UPSTREAM FAILED mart.product_performance UUUUUUU
UPSTREAM FAILED mart.variant_profitability UUUUUUUUU


bruin run completed with failures in 13.545s

 ✗ Assets executed      1 failed due to checks / 12 succeeded / 2 skipped
 ✗ Quality checks       1 failed / 5 succeeded / 16 skipped


1 assets failed
└── stg.product_variants
    └── validate product variant sizes custom check - custom check 'validate product variant sizes' has returned 36 instead of the expected 0

```

::

:::

::

## 10) **Conclusion & Next Steps**

You’ve just built a fully operational, production-style **e-commerce analytics pipeline** using **Bruin**, **Postgres**, and **DuckDB** — from ingestion to quality validation and final metrics delivery.

At this stage, you should now be comfortable with:

* Defining **data generation scripts** that mirror real-world volume and variety.
* Using **Bruin assets** (`.asset.yml`, `.asset.sql`) to model ingestion, staging, and mart layers.
* Declaring **data quality rules** and custom validation queries.
* Executing and debugging pipelines locally with the **Bruin CLI**.


**Where to Go From Here:**

::steps{level="4"}

#### Integrate Chaos Data

Re-run your generator with a `--chaos-percent` above zero to inject controlled invalid data.
You can then observe how **staging and custom checks** detect anomalies automatically — this is your first taste of data observability in practice.

```bash
uv run generate_data.py --dsn postgresql://postgres:postgres@localhost:5432/postgres --chaos-percent 10
bruin run orders-performance --tag staging
```


#### Add Incremental Loads

Refactor your ingestion assets to use `WHERE updated_at >= last_ingest_time`.
Bruin supports parameterized variables and time windows for efficient, repeatable data refreshes.

####  Extend the Mart Layer

Experiment with new KPIs:

* **Repeat Purchase Rate** → joining `orders` with `customers`
* **Top Revenue Categories** → grouping by `product.category`
* **Profitability Over Time** → time-window analysis of the `mart.variant_profitability` table
::

Don't forget to **share your thoughts** or any **issue** with the community at our [Slack Community](#)!
